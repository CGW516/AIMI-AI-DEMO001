# AI è¯­éŸ³ç›´æ’­å¸¦è´§åŠ©æ‰‹ - å®Œæ•´éƒ¨ç½²æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
2. [ä¾èµ–å®‰è£…](#ä¾èµ–å®‰è£…)
3. [å¼¹å¹•æŠ“å–é…ç½®](#å¼¹å¹•æŠ“å–é…ç½®)
4. [LLM API é…ç½®](#llm-api-é…ç½®)
5. [TTS å¼•æ“é…ç½®](#tts-å¼•æ“é…ç½®)
6. [è™šæ‹Ÿå£°å¡é…ç½®](#è™šæ‹Ÿå£°å¡é…ç½®)
7. [OBS æ¨æµé…ç½®](#obs-æ¨æµé…ç½®)
8. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Windows 10/11, macOS, Linux
- **Python**: 3.9+
- **å†…å­˜**: æœ€ä½ 4GBï¼Œæ¨è 8GB
- **æ˜¾å¡**: TTS ä½¿ç”¨ GPT-SoVITS æ—¶éœ€è¦ NVIDIA GPU

### å¿…å¤‡è½¯ä»¶
```bash
# 1. Python ç¯å¢ƒ
python --version  # ç¡®ä¿ >= 3.9

# 2. Gitï¼ˆç”¨äºå…‹éš†é¡¹ç›®ï¼‰
git --version

# 3. OBS Studioï¼ˆç”¨äºæ¨æµï¼‰
# ä¸‹è½½åœ°å€: https://obsproject.com/

# 4. VB-CABLEï¼ˆè™šæ‹Ÿå£°å¡ï¼‰
# ä¸‹è½½åœ°å€: https://vb-audio.com/Cable/
```

---

## ğŸ“¦ ä¾èµ–å®‰è£…

### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir ai_live_assistant
cd ai_live_assistant

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate
```

### å®‰è£…æ ¸å¿ƒä¾èµ–
```bash
# requirements.txt
pip install asyncio
pip install aiohttp        # å¼‚æ­¥ HTTP è¯·æ±‚
pip install websockets     # WebSocket æ”¯æŒ
pip install edge-tts       # Edge TTSï¼ˆæ¨èï¼Œå…è´¹ï¼‰
pip install pyaudio        # éŸ³é¢‘æ’­æ”¾
pip install anthropic      # Claude APIï¼ˆå¦‚æœä½¿ç”¨ï¼‰
pip install openai         # OpenAI APIï¼ˆå¦‚æœä½¿ç”¨ï¼‰
pip install numpy          # éŸ³é¢‘å¤„ç†
pip install pydub          # éŸ³é¢‘æ ¼å¼è½¬æ¢
```

### å¯é€‰ä¾èµ–ï¼ˆé«˜çº§åŠŸèƒ½ï¼‰
```bash
# GPT-SoVITSï¼ˆæœ¬åœ° TTSï¼Œéœ€è¦ GPUï¼‰
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
git clone https://github.com/RVC-Boss/GPT-SoVITS.git

# å‘é‡æ•°æ®åº“ï¼ˆç”¨äº RAGï¼‰
pip install chromadb       # è½»é‡çº§å‘é‡åº“
pip install faiss-cpu      # Facebook å‘é‡æ£€ç´¢

# è‡ªç„¶è¯­è¨€å¤„ç†
pip install jieba          # ä¸­æ–‡åˆ†è¯
```

---

## ğŸ“¡ å¼¹å¹•æŠ“å–é…ç½®

### æ–¹æ¡ˆ A: ä½¿ç”¨ dy-barrage-grabï¼ˆæ¨èï¼‰
```bash
# 1. å®‰è£… Node.js
# ä¸‹è½½åœ°å€: https://nodejs.org/

# 2. å…‹éš†é¡¹ç›®
git clone https://github.com/your-repo/dy-barrage-grab.git
cd dy-barrage-grab
npm install

# 3. é…ç½®ç›´æ’­é—´
# ç¼–è¾‘ config.json
{
  "room_id": "ä½ çš„ç›´æ’­é—´ID",
  "websocket_port": 8080
}

# 4. å¯åŠ¨æœåŠ¡
npm start
```

### æ–¹æ¡ˆ B: ä½¿ç”¨ Python WebSocket ç›´è¿
```python
# barrage_client.py
import asyncio
import websockets
import json

async def connect_barrage():
    uri = "ws://localhost:8080"
    async with websockets.connect(uri) as websocket:
        async for message in websocket:
            data = json.loads(message)
            if data['type'] == 'chat':
                print(f"{data['username']}: {data['content']}")
                # è°ƒç”¨ä¸»ç¨‹åºå¤„ç†
                assistant.handle_message(data['content'], data['username'])

asyncio.run(connect_barrage())
```

---

## ğŸ¤– LLM API é…ç½®

### é€‰é¡¹ 1: Anthropic Claudeï¼ˆæ¨èï¼‰
```python
# config.py
import os

LLM_CONFIG = {
    "provider": "anthropic",
    "api_key": os.getenv("ANTHROPIC_API_KEY"),  # è®¾ç½®ç¯å¢ƒå˜é‡
    "model": "claude-sonnet-4-20250514",
    "max_tokens": 200,
    "temperature": 0.7
}

# è°ƒç”¨ç¤ºä¾‹
import anthropic

client = anthropic.Anthropic(api_key=LLM_CONFIG["api_key"])

async def call_llm(prompt: str):
    with client.messages.stream(
        model=LLM_CONFIG["model"],
        max_tokens=LLM_CONFIG["max_tokens"],
        messages=[{"role": "user", "content": prompt}],
    ) as stream:
        for text in stream.text_stream:
            print(text, end="", flush=True)
            # å®æ—¶å‘é€ç»™ TTS
            await tts_engine.synthesize_chunk(text)
```

### é€‰é¡¹ 2: å›½äº§å¤§æ¨¡å‹ï¼ˆä½æˆæœ¬ï¼‰
```python
# é€šä¹‰åƒé—® / æ–‡å¿ƒä¸€è¨€ / DeepSeek
LLM_CONFIG = {
    "provider": "qwen",
    "api_key": "your_api_key",
    "model": "qwen-turbo",
    "endpoint": "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"
}
```

### ç¯å¢ƒå˜é‡è®¾ç½®
```bash
# Windows PowerShell
$env:ANTHROPIC_API_KEY="sk-ant-..."

# macOS/Linux
export ANTHROPIC_API_KEY="sk-ant-..."

# æ°¸ä¹…è®¾ç½®ï¼ˆæ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrcï¼‰
echo 'export ANTHROPIC_API_KEY="sk-ant-..."' >> ~/.bashrc
```

---

## ğŸ”Š TTS å¼•æ“é…ç½®

### æ–¹æ¡ˆ A: Edge-TTSï¼ˆæ¨èæ–°æ‰‹ï¼‰
**ä¼˜ç‚¹**: å…è´¹ã€æ— éœ€ GPUã€éŸ³è´¨å¥½  
**ç¼ºç‚¹**: éœ€è¦ç½‘ç»œè¿æ¥

```python
import edge_tts
import asyncio

async def synthesize(text: str, output_file: str):
    communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
    await communicate.save(output_file)

# æµå¼åˆæˆï¼ˆè¾¹ç”Ÿæˆè¾¹æ’­æ”¾ï¼‰
async def stream_tts(text: str):
    communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
    async for chunk in communicate.stream():
        if chunk["type"] == "audio":
            # ç›´æ¥å‘é€åˆ°éŸ³é¢‘æ’­æ”¾å™¨
            play_audio_chunk(chunk["data"])
```

**å¯ç”¨è¯­éŸ³åˆ—è¡¨**:
```bash
# æŸ¥çœ‹æ‰€æœ‰ä¸­æ–‡è¯­éŸ³
edge-tts --list-voices | grep zh-CN

# æ¨èè¯­éŸ³
- zh-CN-XiaoxiaoNeural (å¥³å£°ï¼Œæ¸©æŸ”)
- zh-CN-YunxiNeural (ç”·å£°ï¼Œç£æ€§)
- zh-CN-XiaoyiNeural (å¥³å£°ï¼Œæ´»æ³¼)
```

### æ–¹æ¡ˆ B: GPT-SoVITSï¼ˆæ¨èè¿›é˜¶ï¼‰
**ä¼˜ç‚¹**: å£°éŸ³å…‹éš†ã€éŸ³è´¨æœ€ä½³  
**ç¼ºç‚¹**: éœ€è¦ GPUã€éœ€è¦è®­ç»ƒ

```bash
# 1. å…‹éš†é¡¹ç›®
git clone https://github.com/RVC-Boss/GPT-SoVITS.git
cd GPT-SoVITS

# 2. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 3. å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆä½ çš„å£°éŸ³å½•éŸ³ï¼‰
# - å½•åˆ¶ 5-10 åˆ†é’Ÿæ¸…æ™°éŸ³é¢‘
# - åˆ‡åˆ†ä¸º 5-10 ç§’ç‰‡æ®µ
# - æ ‡æ³¨æ–‡æœ¬

# 4. è®­ç»ƒæ¨¡å‹ï¼ˆéœ€è¦ 2-4 å°æ—¶ï¼‰
python train.py --data_path ./data

# 5. å¯åŠ¨æ¨ç†æœåŠ¡
python api.py --port 9880
```

**API è°ƒç”¨**:
```python
import aiohttp

async def gpt_sovits_tts(text: str):
    async with aiohttp.ClientSession() as session:
        async with session.post(
            "http://localhost:9880",
            json={
                "text": text,
                "text_language": "zh",
                "ref_audio_path": "reference.wav"  # å‚è€ƒéŸ³é¢‘
            }
        ) as response:
            audio_data = await response.read()
            return audio_data
```

---

## ğŸ§ è™šæ‹Ÿå£°å¡é…ç½®

### Windows: VB-CABLE
```bash
# 1. ä¸‹è½½å¹¶å®‰è£…
https://vb-audio.com/Cable/

# 2. é‡å¯ç”µè„‘

# 3. éªŒè¯å®‰è£…
# æ‰“å¼€ "å£°éŸ³è®¾ç½®" -> åº”è¯¥çœ‹åˆ° "CABLE Input" å’Œ "CABLE Output"
```

### macOS: BlackHole
```bash
# ä½¿ç”¨ Homebrew å®‰è£…
brew install blackhole-2ch

# æˆ–ä»å®˜ç½‘ä¸‹è½½
https://existential.audio/blackhole/
```

### åœ¨ä»£ç ä¸­æŒ‡å®šè®¾å¤‡
```python
import pyaudio

def get_virtual_device_index():
    p = pyaudio.PyAudio()
    for i in range(p.get_device_count()):
        info = p.get_device_info_by_index(i)
        if "CABLE Input" in info['name']:  # Windows
            return i
        if "BlackHole" in info['name']:     # macOS
            return i
    return None

# ä½¿ç”¨è™šæ‹Ÿè®¾å¤‡
device_index = get_virtual_device_index()
stream = p.open(
    format=pyaudio.paInt16,
    channels=1,
    rate=24000,
    output=True,
    output_device_index=device_index
)
```

---

## ğŸ“¹ OBS æ¨æµé…ç½®

### æ·»åŠ éŸ³é¢‘æº
```
1. æ‰“å¼€ OBS Studio
2. åœ¨ "éŸ³é¢‘æ··éŸ³å™¨" åŒºåŸŸç‚¹å‡» "+"
3. é€‰æ‹© "éŸ³é¢‘è¾“å…¥æ•è·"
4. è®¾å¤‡é€‰æ‹©: "CABLE Output" (Windows) æˆ– "BlackHole 2ch" (macOS)
5. ç¡®å®š
```

### æµ‹è¯•éŸ³é¢‘æµ
```python
# test_audio.py
import edge_tts
import asyncio

async def test():
    text = "å¤§å®¶å¥½ï¼Œæˆ‘æ˜¯ AI ä¸»æ’­å°åŠ©æ‰‹ï¼"
    communicate = edge_tts.Communicate(text, "zh-CN-XiaoxiaoNeural")
    await communicate.save("test.mp3")
    
    # æ’­æ”¾åˆ°è™šæ‹Ÿå£°å¡
    import pygame
    pygame.mixer.init()
    pygame.mixer.music.load("test.mp3")
    pygame.mixer.music.play()
    while pygame.mixer.music.get_busy():
        await asyncio.sleep(0.1)

asyncio.run(test())
```

### æ¨æµè®¾ç½®
```
1. OBS -> è®¾ç½® -> ä¸²æµ
2. æœåŠ¡: é€‰æ‹©ä½ çš„å¹³å°ï¼ˆæŠ–éŸ³/å¿«æ‰‹/Bç«™ï¼‰
3. æœåŠ¡å™¨: rtmp://...
4. ä¸²æµå¯†é’¥: ä»ç›´æ’­å¹³å°è·å–
5. å¼€å§‹æ¨æµ
```

---

## âš™ï¸ å®Œæ•´å¯åŠ¨æµç¨‹

### 1. å¯åŠ¨å¼¹å¹•æœåŠ¡
```bash
cd dy-barrage-grab
npm start
# ç»ˆç«¯åº”æ˜¾ç¤º: WebSocket server running on port 8080
```

### 2. å¯åŠ¨ TTS æœåŠ¡ï¼ˆå¦‚æœä½¿ç”¨ GPT-SoVITSï¼‰
```bash
cd GPT-SoVITS
python api.py --port 9880
# ç»ˆç«¯åº”æ˜¾ç¤º: TTS server running on http://localhost:9880
```

### 3. å¯åŠ¨ä¸»ç¨‹åº
```bash
cd ai_live_assistant
python main.py
```

### 4. å¯åŠ¨ OBS æ¨æµ
```
ç‚¹å‡» "å¼€å§‹æ¨æµ"
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: PyAudio å®‰è£…å¤±è´¥
```bash
# Windows
pip install pipwin
pipwin install pyaudio

# macOS
brew install portaudio
pip install pyaudio

# Linux
sudo apt-get install python3-pyaudio
```

### Q2: å¬ä¸åˆ° AI å£°éŸ³
```bash
# æ£€æŸ¥è™šæ‹Ÿå£°å¡
python -c "import pyaudio; p=pyaudio.PyAudio(); [print(i, p.get_device_info_by_index(i)['name']) for i in range(p.get_device_count())]"

# åº”è¯¥çœ‹åˆ° CABLE æˆ– BlackHole è®¾å¤‡
```

### Q3: LLM å“åº”å¤ªæ…¢
```python
# ä¼˜åŒ–ç­–ç•¥
1. å‡å°‘ max_tokens (200 -> 100)
2. ä½¿ç”¨æ›´å¿«çš„æ¨¡å‹ (GPT-4 -> GPT-3.5)
3. å¯ç”¨ç¼“å­˜ï¼ˆç›¸åŒé—®é¢˜ç›´æ¥è¿”å›ï¼‰
```

### Q4: TTS å»¶è¿Ÿè¿‡é«˜
```python
# ä½¿ç”¨å¥çº§åˆ‡åˆ†
async def stream_response(text: str):
    sentences = text.split('ã€‚')
    for sentence in sentences:
        if sentence.strip():
            await tts_engine.synthesize(sentence + 'ã€‚')
            # ä¸ç­‰å¾…æ’­æ”¾å®Œæˆï¼Œç«‹å³å¤„ç†ä¸‹ä¸€å¥
```

### Q5: å¼¹å¹•æŠ“å–å¤±è´¥
```bash
# æ£€æŸ¥ç›´æ’­é—´æ˜¯å¦å¼€æ’­
# æ£€æŸ¥ç½‘ç»œè¿æ¥
# æŸ¥çœ‹æ§åˆ¶å°é”™è¯¯æ—¥å¿—

# ä½¿ç”¨ä»£ç†ï¼ˆå¦‚æœè¢«é™æµï¼‰
export HTTP_PROXY="http://127.0.0.1:7890"
export HTTPS_PROXY="http://127.0.0.1:7890"
```

---

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–å»ºè®®

### ä½å»¶è¿Ÿé…ç½®
```python
CONFIG = {
    "llm_max_tokens": 80,           # å‡å°‘ç”Ÿæˆé•¿åº¦
    "tts_sentence_split": True,     # å¥çº§åˆæˆ
    "audio_buffer_size": 1024,      # å°ç¼“å†²åŒº
    "concurrent_requests": 3,       # å¹¶å‘å¤„ç†
}
```

### é«˜è´¨é‡é…ç½®
```python
CONFIG = {
    "llm_model": "claude-opus-4",   # æ›´å¼ºå¤§çš„æ¨¡å‹
    "tts_engine": "gpt-sovits",     # å£°éŸ³å…‹éš†
    "audio_sample_rate": 48000,     # é«˜é‡‡æ ·ç‡
}
```

---

## ğŸš€ è¿›é˜¶åŠŸèƒ½

### å¤šè½®å¯¹è¯è®°å¿†
```python
class ConversationMemory:
    def __init__(self, max_turns=10):
        self.history = []
        self.max_turns = max_turns
    
    def add_turn(self, user_msg, ai_msg):
        self.history.append({"user": user_msg, "ai": ai_msg})
        if len(self.history) > self.max_turns:
            self.history.pop(0)
    
    def get_context(self):
        return "\n".join([f"ç”¨æˆ·: {t['user']}\nAI: {t['ai']}" for t in self.history])
```

### æƒ…æ„Ÿæ£€æµ‹
```python
def detect_emotion(text: str) -> str:
    positive = ["å¥½", "å–œæ¬¢", "ä¸é”™", "æ£’"]
    negative = ["å·®", "ä¸å¥½", "åƒåœ¾"]
    
    if any(word in text for word in positive):
        return "positive"
    elif any(word in text for word in negative):
        return "negative"
    return "neutral"
```

### è‡ªåŠ¨ä¸Šæ¶æé†’
```python
async def product_reminder():
    while True:
        await asyncio.sleep(300)  # æ¯ 5 åˆ†é’Ÿ
        await tts_engine.synthesize("æ–°å“ä¸Šæ¶ï¼ç°åœ¨ä¸‹å•è¿˜æœ‰é¢å¤–ä¼˜æƒ ï¼")
```

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

- **æ–‡æ¡£**: [https://docs.example.com](https://docs.example.com)
- **GitHub**: [https://github.com/your-repo](https://github.com/your-repo)
- **Discord**: [https://discord.gg/...](https://discord.gg/...)

---

**ç¥ä½ æ­å»ºé¡ºåˆ©ï¼ğŸ‰**